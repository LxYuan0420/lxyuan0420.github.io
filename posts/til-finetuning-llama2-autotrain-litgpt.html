<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Lx | TIL: Finetuning Llama-2 Model with Autotrain and Lit-GPT</title>
  <meta name="description" content="This article describe how to finetune the Llama-2 Model with two APIs">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="TIL: Finetuning Llama-2 Model with Autotrain and Lit-GPT">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://lxyuan0420.github.io/posts/til-finetuning-llama2-autotrain-litgpt">
  <meta property="og:description" content="This article describe how to finetune the Llama-2 Model with two APIs">
  <meta property="og:site_name" content="Lx">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="https://lxyuan0420.github.io/posts/til-finetuning-llama2-autotrain-litgpt">
  <meta name="twitter:title" content="TIL: Finetuning Llama-2 Model with Autotrain and Lit-GPT">
  <meta name="twitter:description" content="This article describe how to finetune the Llama-2 Model with two APIs">

  
    <meta property="og:image" content="https://lxyuan0420.github.io/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
    <meta name="twitter:image" content="https://lxyuan0420.github.io/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
  

  <link href="https://lxyuan0420.github.io/feed.xml" type="application/rss+xml" rel="alternate" title="Lx Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-bb1553a18d0f1ccfe1aabc010584c49b4277a88503216b78906ba719e30019c1.css">
    

  

</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav scrollappear">
  <a href="/" class="header-logo" title="Lx">Lx</a>
  <ul class="header-links">
    
    
      <li>
        <a href="https://twitter.com/lxyuan" rel="noreferrer noopener" target="_blank" title="Twitter">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-twitter">
  <use href="/assets/twitter-8842c33965263ad1b03a978406826677a668f94125d5837e70ab83f24b3213a7.svg#icon-twitter" xlink:href="/assets/twitter-8842c33965263ad1b03a978406826677a668f94125d5837e70ab83f24b3213a7.svg#icon-twitter"></use>
</svg>

        </a>
      </li>
    
    
    
    
      <li>
        <a href="https://github.com/LxYuan0420" rel="noreferrer noopener" target="_blank" title="GitHub">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-github">
  <use href="/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github" xlink:href="/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
      <li>
        <a href="https://www.linkedin.com/in/likxunyuan" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-linkedin">
  <use href="/assets/linkedin-cdc5c107044324a3dfbea2e9ead15873f8dee304c37d73a046988956b706256e.svg#icon-linkedin" xlink:href="/assets/linkedin-cdc5c107044324a3dfbea2e9ead15873f8dee304c37d73a046988956b706256e.svg#icon-linkedin"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="mailto:lxyuan0420@gmail.com" title="Email">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-email">
  <use href="/assets/email-782473193bf750036fdb90e8daa075508a20509d01854c09f3237c144a3f0601.svg#icon-email" xlink:href="/assets/email-782473193bf750036fdb90e8daa075508a20509d01854c09f3237c144a3f0601.svg#icon-email"></use>
</svg>

        </a>
      </li>
    
    
    
  </ul>
</nav>



        <article class="article scrollappear">
          <header class="article-header">
            <h1>TIL: Finetuning Llama-2 Model with Autotrain and Lit-GPT</h1>
            <p>This article describe how to finetune the Llama-2 Model with two APIs</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    July 22, 2023
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      3 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
  </div>
</div>
          </header>

          <div class="article-content">
            <p>Today, I’m going to share what I learned about fine-tuning the Llama-2 model
using two distinct APIs: autotrain-advanced from Hugging Face and Lit-GPT from
Lightning AI. This guide will be a blend of technical precision and
straightforward instructions, peppered with code examples to make the process
as clear as possible.</p>

<h4 id="fine-tuning-with-autotrain-advanced-from-hugging-face">Fine-Tuning with autotrain-advanced from Hugging Face</h4>
<p>Hugging Face’s autotrain-advanced is a powerful tool that simplifies the
process of fine-tuning models. Here’s a step-by-step guide on how to use it
with the Llama-2 model:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># install the autotrain-advanced package and update PyTorch
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">autotrain</span><span class="o">-</span><span class="n">advanced</span> <span class="o">--</span><span class="n">quiet</span>
<span class="err">!</span><span class="n">autotrain</span> <span class="n">setup</span> <span class="o">--</span><span class="n">update</span><span class="o">-</span><span class="n">torch</span>
<span class="c1"># fine-tune the Llama-2 model
</span><span class="err">!</span><span class="n">autotrain</span> <span class="n">llm</span> <span class="o">--</span><span class="n">train</span> \
    <span class="o">--</span><span class="n">project_name</span> <span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">finetuned</span><span class="o">-</span><span class="n">alpaca</span> \
    <span class="o">--</span><span class="n">data_path</span> <span class="n">tatsu</span><span class="o">-</span><span class="n">lab</span><span class="o">/</span><span class="n">alpaca</span> \
    <span class="o">--</span><span class="n">model</span> <span class="n">abhishek</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">hf</span><span class="o">-</span><span class="n">small</span><span class="o">-</span><span class="n">shards</span> \
    <span class="o">--</span><span class="n">learning_rate</span> <span class="mf">2e-4</span> \
    <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">3</span> \
    <span class="o">--</span><span class="n">train_batch_size</span> <span class="mi">4</span> \
    <span class="o">--</span><span class="n">eval_batch_size</span> <span class="mi">4</span> \
    <span class="o">--</span><span class="n">gradient_accumulation_steps</span> <span class="mi">32</span> \
    <span class="o">--</span><span class="n">use_peft</span> \
    <span class="o">--</span><span class="n">use_int4</span> \
    <span class="o">--</span><span class="n">trainer</span> <span class="n">sft</span> \
    <span class="o">--</span><span class="n">push_to_hub</span> \
    <span class="o">--</span><span class="n">save_total_limit</span> <span class="mi">2</span> \
    <span class="o">--</span><span class="n">repo_id</span> <span class="n">lxyuan</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">alpaca</span></code></pre></figure>

<p>This command will fine-tune the Llama-2 model on the Alpaca dataset from Tatsu
Lab, using a learning rate of 2e-4, a batch size of 4 for both training and
evaluation, and a gradient accumulation step of 32. The fine-tuned model will
be saved in the specified repository on Hugging Face’s Model Hub.</p>

<p>The command above takes about 175 hours to complete training on a T4 GPU (free
colab use T4 too). I’m currently experiencing difficulties with my Google Cloud
Compute instance due to an error message that indicates the unavailability of
an n1-standard-8 VM instance in the asia-southeast1-b zone. Given this
situation, I plan to postpone my model training task to sometime next week. At
that time, I will be attempting the training on an A100 machine instead.</p>

<h4 id="fine-tuning-with-lit-gpt-from-lightning-ai">Fine-Tuning with Lit-GPT from Lightning AI</h4>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Download the model weights 
</span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">download</span><span class="p">.</span><span class="n">py</span> \ 
    <span class="o">--</span><span class="n">repo_id</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">hf</span> 

<span class="c1"># Convert the weights to Lit-GPT format 
</span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">convert_hf_checkpoint</span><span class="p">.</span><span class="n">py</span> \ 
    <span class="o">--</span><span class="n">checkpoint_dir</span> <span class="n">checkpoints</span><span class="o">/</span><span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">hf</span> 

<span class="c1"># Prepare the dataset 
# check out this script: https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/prepare_alpaca.py
# to modify the Alpaca script, open `prepare_alpaca.py` and edit the prepare function. 
</span><span class="n">python</span> <span class="n">scripts</span><span class="o">/</span><span class="n">prepare_alpaca</span><span class="p">.</span><span class="n">py</span> \ 
    <span class="o">--</span><span class="n">destination_path</span> <span class="n">data</span><span class="o">/</span><span class="n">dolly</span> \ 
    <span class="o">--</span><span class="n">checkpoint_dir</span> <span class="n">checkpoints</span><span class="o">/</span><span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">hf</span> 

<span class="c1"># Finetune Llama 2 on custom dataset 
</span><span class="n">python</span> <span class="n">finetune</span><span class="o">/</span><span class="n">lora</span><span class="p">.</span><span class="n">py</span> \ 
    <span class="o">--</span><span class="n">data_dir</span> <span class="n">data</span><span class="o">/</span><span class="n">dolly</span> \ 
    <span class="o">--</span><span class="n">checkpoint_dir</span> <span class="n">checkpoints</span><span class="o">/</span><span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">hf</span> </code></pre></figure>

<p>For Step-3 of preparing the datset, we suggest reader to go thru this <a href="https://lightning.ai/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/">blog</a> and this python <a href="https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/prepare_alpaca.py#L27">script</a> first to understand how to prepare your custom dataset.</p>

<p>That’s it!</p>

<p>Related:</p>
<ul>
  <li><a href="https://twitter.com/abhi1thakur/status/1681641346396835841">LLAMA-v2 training successfully on Google Colab’s free version! “pip install autotrain-advanced”</a></li>
  <li><a href="https://www.youtube.com/watch?v=3fsn19OI_C8">The EASIEST way to finetune LLAMA-v2 on local machine!</a></li>
  <li><a href="https://lightning.ai/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/">How To Finetune GPT Like Large Language Models on a Custom Dataset</a></li>
  <li><a href="https://twitter.com/LightningAI/status/1682392395655118848">Finetune Llama 2 on a custom dataset in 4 steps using Lit-GPT.</a></li>
</ul>

          </div>
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=TIL:+Finetuning+Llama-2+Model+with+Autotrain+and+Lit-GPT%20-%20https://lxyuan0420.github.io/posts/til-finetuning-llama2-autotrain-litgpt%20by%20@lxyuan" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://lxyuan0420.github.io/posts/til-finetuning-llama2-autotrain-litgpt" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>

          
        </article>
        <footer class="footer scrollappear">
  <p>
    Made with <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">Chalk</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  

<script src="/assets/vendor-3ee2c63bbac916f96cd7f90e83ab767f058ead1301444c9966f5156911c8be7f.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>



  <script src="/assets/scrollappear-e2da8ea567e418637e31266cc5302126eaa79f62a2273739086358b589a89ee6.js" type="text/javascript"></script>


<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


</body>
</html>
