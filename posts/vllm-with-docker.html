<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Lx | Deploying vLLMs with Docker: A Guide to Fixing NVIDIA GPU Access</title>
  <meta name="description" content="A straightforward guide to resolving the 'unknown or invalid runtime name: nvidia' error for developers deploying VLLM using Docker.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Deploying vLLMs with Docker: A Guide to Fixing NVIDIA GPU Access">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://lxyuan0420.github.io/posts/vllm-with-docker">
  <meta property="og:description" content="A straightforward guide to resolving the 'unknown or invalid runtime name: nvidia' error for developers deploying VLLM using Docker.">
  <meta property="og:site_name" content="Lx">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="https://lxyuan0420.github.io/posts/vllm-with-docker">
  <meta name="twitter:title" content="Deploying vLLMs with Docker: A Guide to Fixing NVIDIA GPU Access">
  <meta name="twitter:description" content="A straightforward guide to resolving the 'unknown or invalid runtime name: nvidia' error for developers deploying VLLM using Docker.">

  
    <meta property="og:image" content="https://lxyuan0420.github.io/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
    <meta name="twitter:image" content="https://lxyuan0420.github.io/assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
  

  <link href="https://lxyuan0420.github.io/feed.xml" type="application/rss+xml" rel="alternate" title="Lx Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-a98c41efc5ed9fcc06ac664c9e2f7a9b3c3b2e0a52357d221fe382f6f4abc8fc.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-bb1553a18d0f1ccfe1aabc010584c49b4277a88503216b78906ba719e30019c1.css">
    

  

</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav scrollappear">
  <a href="/" class="header-logo" title="Lx">Lx</a>
  <ul class="header-links">
    
    
      <li>
        <a href="https://twitter.com/lxyuan" rel="noreferrer noopener" target="_blank" title="Twitter">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-twitter">
  <use href="/assets/twitter-8842c33965263ad1b03a978406826677a668f94125d5837e70ab83f24b3213a7.svg#icon-twitter" xlink:href="/assets/twitter-8842c33965263ad1b03a978406826677a668f94125d5837e70ab83f24b3213a7.svg#icon-twitter"></use>
</svg>

        </a>
      </li>
    
    
    
    
      <li>
        <a href="https://github.com/LxYuan0420" rel="noreferrer noopener" target="_blank" title="GitHub">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-github">
  <use href="/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github" xlink:href="/assets/github-094f81040819f34343ee6ffff0980f17e2807b08b595eaaf66ae3554934fd78d.svg#icon-github"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
      <li>
        <a href="https://www.linkedin.com/in/likxunyuan" rel="noreferrer noopener" target="_blank" title="LinkedIn">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-linkedin">
  <use href="/assets/linkedin-cdc5c107044324a3dfbea2e9ead15873f8dee304c37d73a046988956b706256e.svg#icon-linkedin" xlink:href="/assets/linkedin-cdc5c107044324a3dfbea2e9ead15873f8dee304c37d73a046988956b706256e.svg#icon-linkedin"></use>
</svg>

        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="mailto:lxyuan0420@gmail.com" title="Email">
          <svg xmlns="http://www.w3.org/2000/svg" class="icon-email">
  <use href="/assets/email-782473193bf750036fdb90e8daa075508a20509d01854c09f3237c144a3f0601.svg#icon-email" xlink:href="/assets/email-782473193bf750036fdb90e8daa075508a20509d01854c09f3237c144a3f0601.svg#icon-email"></use>
</svg>

        </a>
      </li>
    
    
    
  </ul>
</nav>



        <article class="article scrollappear">
          <header class="article-header">
            <h1>Deploying vLLMs with Docker: A Guide to Fixing NVIDIA GPU Access</h1>
            <p>A straightforward guide to resolving the 'unknown or invalid runtime name: nvidia' error for developers deploying VLLM using Docker.</p>
            <div class="article-list-footer">
  <span class="article-list-date">
    April 1, 2024
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      3 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
  </div>
</div>
          </header>

          <div class="article-content">
            <p>vLLM, a high-throughput and memory-efficient inference engine, makes serving
large language models like LLaMA-2 straightforward, even on diverse platforms
like ROCm and various clouds via SkyPilot. This guide documents a Docker
deployment issue related to NVIDIA GPU access and provides a detailed solution,
enhancing the vLLM serving experience.</p>

<p>Deploying vLLMs with Docker service requires NVIDIA GPU support, but Docker can sometimes fail to access the GPU, showing errors. This guide resolves such issues, ensuring vLLM can utilize the necessary GPU resources.</p>

<h3 id="problem">Problem</h3>

<p>While deploying <a href="https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html">vLLM</a> with the following Docker command:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker run <span class="nt">--runtime</span> nvidia <span class="nt">--gpus</span> all ...</code></pre></figure>

<p>I was met with:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker: Error response from daemon: unknown or invalid runtime name: nvidia.</code></pre></figure>

<p>Removing <code class="highlighter-rouge">--runtime nvidia</code> led to a new error about the inability to find a
device driver with GPU capabilities.</p>

<h3 id="resolution-steps">Resolution Steps:</h3>

<h4 id="verifying-nvidia-docker2-installation">Verifying nvidia-docker2 Installation</h4>

<p>It’s crucial to have <code class="highlighter-rouge">nvidia-docker2</code> installed for Docker to interface with
NVIDIA GPUs. Begin by verifying its presence:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Check if nvidia-docker2 is installed</span>
dpkg <span class="nt">-l</span> | <span class="nb">grep </span>nvidia-docker
docker info | <span class="nb">grep </span>nvidia</code></pre></figure>

<h4 id="installing-nvidia-docker2">Installing nvidia-docker2</h4>

<p>If nvidia-docker2 is missing, install it to bridge Docker with NVIDIA GPUs:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Update package lists and install nvidia-docker2</span>
<span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> nvidia-docker2

<span class="c"># Restart Docker to apply changes</span>
<span class="nb">sudo </span>systemctl restart docker</code></pre></figure>

<h4 id="confirming-the-installation">Confirming the Installation</h4>

<p>Ensure the installation was successful by inspecting Docker’s runtime
configuration and checking the installed version:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Confirm Docker's runtime configuration for NVIDIA</span>
<span class="nb">cat</span> /etc/docker/daemon.json

<span class="o">&gt;&gt;&gt;</span> <span class="o">{</span>
    <span class="s2">"runtimes"</span>: <span class="o">{</span>
        <span class="s2">"nvidia"</span>: <span class="o">{</span>
            <span class="s2">"path"</span>: <span class="s2">"nvidia-container-runtime"</span>,
            <span class="s2">"runtimeArgs"</span>: <span class="o">[]</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="c"># Verify the nvidia-docker2 version</span>
dpkg <span class="nt">-l</span> | <span class="nb">grep </span>nvidia-docker

<span class="c"># Check Docker runtimes for NVIDIA support</span>
docker info | <span class="nb">grep </span>nvidia</code></pre></figure>

<h3 id="successful-vllm-deployment">Successful vLLM Deployment</h3>

<p>With NVIDIA GPU support enabled, execute the Docker command to deploy the vLLM:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Deploy vLLM with NVIDIA GPU support</span>
docker run <span class="nt">--runtime</span> nvidia <span class="nt">--gpus</span> all <span class="se">\</span>
    <span class="nt">-v</span> /home/likxun/.cache/huggingface:/root/.cache/huggingface <span class="se">\</span>
    <span class="nt">--env</span> <span class="s2">"HUGGING_FACE_HUB_TOKEN=</span><span class="si">$(</span><span class="nb">cat</span> /home/likxun/.cache/huggingface/token<span class="si">)</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">-p</span> 8000:8000 <span class="se">\</span>
    <span class="nt">--ipc</span><span class="o">=</span>host <span class="se">\</span>
    vllm/vllm-openai:latest <span class="se">\</span>
    <span class="nt">--model</span> TheBloke/Mistral-7B-Instruct-v0.1-GPTQ <span class="se">\</span>
    <span class="nt">--quantization</span> <span class="s2">"gptq"</span> <span class="se">\</span>
    <span class="nt">--dtype</span> <span class="s2">"half"</span></code></pre></figure>

<h4 id="testing-the-deployment">Testing the Deployment</h4>

<p>Verify that the vLLM is operational by executing a test request:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Test the vLLM deployment</span>
curl http://localhost:8000/v1/chat/completions <span class="se">\</span>
    <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
    <span class="nt">-d</span> <span class="s1">'{
        "model": "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ",
        "messages": [{"role": "user", "content": "What is 2+2?"}]
    }'</span></code></pre></figure>

<h4 id="expected-output">Expected Output:</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="o">{</span>
    <span class="s2">"id"</span>: <span class="s2">"cmpl-afbe2ffa3e0d4779ba28ff8afae5b6a9"</span>,
    <span class="s2">"object"</span>: <span class="s2">"chat.completion"</span>,
    <span class="s2">"created"</span>: 1711946527,
    <span class="s2">"model"</span>: <span class="s2">"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"</span>,
    <span class="s2">"choices"</span>: <span class="o">[</span>
        <span class="o">{</span>
            <span class="s2">"index"</span>: 0,
            <span class="s2">"message"</span>: <span class="o">{</span>
                <span class="s2">"role"</span>: <span class="s2">"assistant"</span>,
                <span class="s2">"content"</span>: <span class="s2">" 2+2 is 4."</span>
            <span class="o">}</span>,
            <span class="s2">"logprobs"</span>: null,
            <span class="s2">"finish_reason"</span>: <span class="s2">"stop"</span>,
            <span class="s2">"stop_reason"</span>: null
        <span class="o">}</span>
    <span class="o">]</span>,
    <span class="s2">"usage"</span>: <span class="o">{</span>
        <span class="s2">"prompt_tokens"</span>: 16,
        <span class="s2">"total_tokens"</span>: 25,
        <span class="s2">"completion_tokens"</span>: 9
    <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

<p>This response indicates that the vLLM deployment is successful and capable of processing requests.</p>

<h3 id="conclusion">Conclusion</h3>

<p>This guide provided a detailed walkthrough for resolving Docker and NVIDIA GPU
integration issues, ensuring a successful vLLM deployment. By following these
steps, users can overcome common hurdles, enabling efficient and effective
model serving with GPU support.</p>


          </div>
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Deploying+vLLMs+with+Docker:+A+Guide+to+Fixing+NVIDIA+GPU+Access%20-%20https://lxyuan0420.github.io/posts/vllm-with-docker%20by%20@lxyuan" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"/></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://lxyuan0420.github.io/posts/vllm-with-docker" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewBox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"/></svg>
            </a>
          </div>

          
        </article>
        <footer class="footer scrollappear">
  <p>
    Made with <a href="https://github.com/nielsenramon/chalk" rel="noreferrer noopener" target="_blank" title="Download Chalk">Chalk</a>.
  </p>
</footer>

      </div>
    </div>
  </main>
  

<script src="/assets/vendor-3ee2c63bbac916f96cd7f90e83ab767f058ead1301444c9966f5156911c8be7f.js" type="text/javascript"></script>


  <script src="/assets/webfonts-96493456d319d1bf419afdf8701552d4d486fee6afd304897d4fd81eb4e0cc0b.js" type="text/javascript"></script>



  <script src="/assets/scrollappear-e2da8ea567e418637e31266cc5302126eaa79f62a2273739086358b589a89ee6.js" type="text/javascript"></script>


<script src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js" type="text/javascript"></script>


</body>
</html>
